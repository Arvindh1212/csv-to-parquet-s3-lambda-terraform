# ğŸ“š CSV to Parquet Data Pipeline on AWS (Terraform + GitHub Actions)

---

## ğŸ“™ Project Overview

This project automates an event-driven data pipeline using AWS services.
When a CSV file is uploaded to the source S3 bucket, it automatically triggers a Lambda function that:

- Converts the CSV file to Parquet format.
- Uploads the Parquet file to the destination S3 bucket.
- Sends a notification email via SNS (Simple Notification Service).

All AWS resources are provisioned and smanaged automatically using Terraform modules and CI/CD with GitHub Actions.

---

## âš¡ Architecture

![Architecture Diagram](./architecture.png)
---

## ğŸ› ï¸ Technologies Used

- **AWS S3** (Source and Destination Buckets)
- **AWS Lambda** (CSV â” Parquet conversion)
- **AWS SNS** (Email Notification)
- **AWS IAM** (Permissions and Role Management)
- **Terraform** (Infrastructure as Code)
- **GitHub Actions** (CI/CD for Terraform)
---

## ğŸ“¦ Project Structure
```
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ terraform.yml         # GitHub Actions CI/CD pipeline
â”œâ”€â”€ modules/
â”‚   â””â”€â”€ s3_lambda_sns/
â”‚       â”œâ”€â”€ main.tf               # Terraform module main configuration
â”‚       â”œâ”€â”€ variables.tf          # Module input variables
â”‚       â””â”€â”€ outputs.tf            # Module outputs (optional)
â”œâ”€â”€ python/
â”‚   â””â”€â”€ lambda_function.py        # Lambda function to convert CSV to Parquet
â”œâ”€â”€ main.tf                       # Root module to call s3_lambda_sns module
â”œâ”€â”€ variables.tf                  # Root variables
â”œâ”€â”€ terraform.tfvars              # Variables values
â””â”€â”€ README.md                     # Project Documentation
```
---

## ğŸš€ Deployment Instructions

### Clone the repository

```bash
git clone https://github.com/your-username/your-repo-name.git
cd your-repo-name
```

### Configure your AWS credentials
*(Make sure you are using a named profile if needed.)*

### Initialize Terraform

```bash
terraform init
```

### Review the plan

```bash
terraform plan
```

### Apply the configuration

```bash
terraform apply -auto-approve
```

### Verify

- Upload a CSV file to the source S3 bucket.
- Check the destination S3 bucket for the converted Parquet file.
- Check your email for the notification.

---

## ğŸ’ª Automated Deployment (CI/CD)

Every push to the GitHub repository triggers GitHub Actions to automatically apply the latest changes.

---

## ğŸŒŸ Features

- ğŸ“¥ **S3 Event Trigger**: Upload CSV files to automatically trigger the Lambda.
- ğŸ”„ **Lambda Conversion**: Lambda function converts CSV to Parquet format.
- ğŸ“¤ **S3 Storage**: Saves converted Parquet files to destination bucket.
- ğŸ“§ **SNS Notifications**: Sends an email notification after successful conversion.
- ğŸ—ï¸ **Infrastructure as Code**: All AWS resources are managed through Terraform.
- âš¡ **Continuous Deployment**: Automated CI/CD with GitHub Actions.


---

---

## ğŸ’¼  Use Cases

- ğŸ§© **Data Lake Ingestion Pipelines:**: Automate conversion of raw CSV data to optimized Parquet files for Big Data processing.
- ğŸ§© **Real-Time Data Processing**: Instantly transform incoming datasets without manual intervention.
- ğŸ§© **Cloud Storage Optimization**: Save storage costs and improve analytics performance by using efficient file formats like Parquet.
- ğŸ§© **SNS Notifications**: Save storage costs and improve analytics performance by using efficient file formats like Parquet.
- ğŸ§© **Automated Reporting Pipelines**: Ensure that data is ready for visualization or Machine Learning workflows immediately after upload.
- ğŸ§© **ETL Workflows**: erve as a lightweight Extract-Transform-Load solution for serverless data operations.


---

## ğŸ“§ Contact

- **Developer:** Arvindh Saravanan
- **Email:** arvindhsk1212@gmail.com
- **GitHub:** https://github.com/Arvindh1212

---

## âœ… Status

- **Project Status:** 100% Completed
- **Working:** Successfully tested and verified.

---


# ğŸ‰ Thank you for reviewing this project!

